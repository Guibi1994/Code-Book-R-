---
title: "Code Book ver 2.0"
author: "Guibor Camargo (guibor.camargo@urosario.edu.co)||"
date: '2022-07-03'
output: github_document
---

```{r librerias_general, echo=FALSE, warning=FALSE, message=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(rvest)
```

# Introducción

Este guía es para aprender nociones básicas e intermedias de análisis de datos **R**. La intención es guíar al lector de manera consitente y resumida por alguas de las principales técnias en ciencia de datos y econometría en **R**. De esta manera, no se asume nada (o casi nada) sobre el lector: es decir, es una guía tanto para principiantes como para usuarios con conocimientos previos en programación y/o estadísticas.

# "Regex" manejo básico

"Regex" es una sintáxis universal de textos. Es básicamente una lenguaje común, en muchos entornos de programación, que se emplea para hacer consultas dentro de un texto o "string". En "regex" cada letra, espacio, simbolo o número es un **caracter independiente** y tiene una manera difernte de consultarse. El conocimiento básico de esta sintáxis para el manejo de textos, es uno de los pilares fundamentales de otras técnicas como "text mining", "web-scraping" y "sentiment analysis" entre otras. Lo mostrado aca se base principalmente en las clases de [análisis de textos del curso edX de la Universidad de Harvard en Data Science](https://rafalab.github.io/dslibro/procesamiento-de-cadenas.html "Contulte acá el capítulo de manejo de textos de ese curso")

```{r, eval=FALSE, echo=TRUE}
# Liberria principal
library(stringr)

# Librerias complementarias
library(dplyr) # Manejo de bases de datos
library(tidyr) # Manejo de bases de datos
```

# Fundamentos de Web-scraping en R

*Web scraping* o web harvesting hace referencia al conjutno de técnicasl empleadas para extraer información en la wed que no esta necesariamente empaquetada en un objeto o archivo consumible de manera directa .

```{r, eval=FALSE, echo=TRUE}

# libreira principal
library(rvest) # Para web scraping

# librerias complementarias
library(dplyr) # Manejo de bases de datos
library(tidyr) # Manejo de bases de datos
library(ggplot2) # Creaicón de gráficas
```

## Estructura de un HTML (lo que hay que saber)

Los documentos **HTML** (*Hypert Text Markup*) son archivos estructurados que contienen una semántica regular, y son los que por lo general podemos observar detrás de las páginas web (la mayoria de ellas). Si dentro de una págian web damos "click secudnario" y luego "inspecionar" (o tambien "**Ctrl+u**"), podemos ver con clairdad el documetno HTML detras de ella.

<p align="center">
<img src="00_Code_Book_2.0_files/figure-gfm/prueba.png" alt="Preuba" width="540"/>
</p>

HTML como lenguaje de porgramación tiene una sintaxis y una estructura lógica. Una muestra de esto es que cada uno de los **elementos** de una documento HTML esta **encapsulado** en un "**nodo**". Por ejemplo, el título de un documento HTML esta codificado así: `<title>"Un gran titulo"</titile>` en donde `<title>` es el nodo del titulo. Esta estructura de "nodos" nos permite detectar patrones y elementos puntuales dentro de estos documentos.

<p align="center">
<img src="00_Code_Book_2.0_files/figure-gfm/prueba2.png" alt="estructura_html" width="484"/>
</p>

Algunos elementos con sus tespectivos "***tags***" (nombre del nodo) comúnes en un documento HTML son:

-   `<!DOCTYPE html>` : Declaración del tipo de docuento (en este caso un HTML)

-   `<head>` : Contiene metadata sobre la pagina HTML

-   `<title>` : El título de la pagina en cuestion (el que finalmente se muestra en el buscador/browser)

-   `<body>` :El contiendo de la página en cuestión.

Otros tags comunes son `<p>`(para denominar parrafos), `<table>`(para denominar tablas), `<img>`(para denominar imágenes), `<map>`(para denóminar mapas), etc... [En el éste enlace puedes ver una lista de tagas comúnes en un documento HTML](https://www.w3schools.com/TAgs/default.asp "Tags de referencia").

## Web scraping de tablas

En este ejemplo utilizaremos la página web de [Wilkipedia del PIB (Prodcucto Interno Bruto) por paises](https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal) "¡Haz click aqui!"), para emplear las técnicas básicas de web scrping en **R**. Los pasos a seguir asumen que ya instalamos y cargamos la libreria Rvest: `library(rvest)`

1.  Extraemos la URL (el *link*) de la página en donde se encutra nuestra tabla

2.  Con la función `read_html` podemos leer el archivo HTML detrás del link. Este será leido como un arichivo tipo XML en forma de *lista* de elementos.

    ```{r, message=FALSE, echo=TRUE}
    PIB_paises <- read_html("https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)")
    ```

3.  El siguiente paso, consite en utilizar el comando `html_nodes` ( OJO "nodes" no "node") para buscar el nodo que nos interesa. En este caso el nodo que nos interesa es `<table>` por que estamos extrallendo directamente una tabla del documento HTML. Basicamente estamos solicitando filtrar todos los nodos y solo quedarnos con aquellos cuyo ***tag*** sea "table", y estos quedara guardados en una *lista.*

    ```{r, message=FALSE, echo=TRUE}
    PIB_paises <- PIB_paises %>% html_nodes("table")
    ```

4.  El siguiente paso puede ser un poco engañoso. Una vez extraigamos el nodo "table", realmente estamos extrallendo no uno, sino todos los nodos llamados "table" presentes en el documento HTML. En nuestro caso, podemos ver cómo una vez extremos el nodo "table" tenemos un total de 7 nodos con ese nombre ¿cuál es el número del nodo que nos interesa?   
      
    Para saberlo podemos inspeccionar la página (**Ctrl+u**) y buscar (**Ctrl+f**) "\<table", y luego deteminar cuál es el número del nodo "table" que da apertura a la tabla que nos interesa extraer. Otra forma es con prueba y error, pero es más efectivo de la primera manera. En nuestro ejemplo (como se ve a continuación), el nodo #3 que encontramos en el documento HTML el que da apertura a la tabla que nos interesa extraer.

    <p align="center">
    <img src="00_Code_Book_2.0_files/figure-gfm/prueba3.png" alt="estructura_html" width="484"/>
    </p>

    En nuestro ejemplo (como se en la imágen anterior), el **3*er* nodo** que encontramos en el documento HTML, es el que da apertura a la tabla que nos interesa extraer.   
      
    :gift: **TIP** :gift:**:** En algunos casos, dentro del código HTML las tablas tiene un "id" despues del *tag*: (ejm) `<table id="Nombre_tabla">` . En estos casos podemos usar el comando `html_nodes("table#Nombre_tabla")` para extraer específicamente la tabla que queremos. Sin embargo en este caso (como un muchos) no existe esta posilidad por la falta de un indentficador específico de la tabla.

5.  Ya sabemos que 3*er* elemento en nuestra lista es el nodo que contiene una **tabla** que nos interesa. Lo siguiente es usar el comando `html_table()` para especificar que es un nodo "tipo tabla", y extraer 3er nodo posteriormente (podemo utilizar la sintáxis básica de R para indicar la consulta del 3er elemento de la lista: `[[3]]` o tambien `[3]` ). Podemos tambien de una vez en el mismo paso converir el resultado a una "data.frame" y/o a un "tible".

    ```{r, message=FALSE, echo=TRUE}
    PIB_paises <- PIB_paises %>% 
      # Leer la/s tabla/s del documento HTML
      html_table() %>%
      # Acceder el 3er elemento de la lista
      .[[3]] %>% 
      # Cambiar a formato de base de datos
      as.data.frame()

    head(PIB_paises,5)
    ```

6.  :recycle: **RESUMEN:** Ya los pasos siguientes son realizar los arreglos correspondientes que se tengan que hacer para cada paso puntual. Pero ya extraimos la información. Incluso podemos hacer todo en una sola línea de código

    ```{r ,message=FALSE,echo=TRUE, warning=FALSE}

    PIB_paises <- 
      # 1. Leer HTML desde la URL
      read_html("https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)") %>% 
      # 2. Extrer nodos llamados "table"
      html_nodes("table") %>% 
      # 3. Convertir a formato tabla
      html_table() %>% 
      # 4. Extaer el elemnto de interes
      .[[3]] %>% 
      # 5. Transformar a una base de datos
      as.data.frame() %>% 
      
      
      # - - - - - - - - - - - -
      # Pasos EXTRA: Limpiar un poco la base (cada caso es particular)
      janitor::row_to_names(row_number = 1) %>% 
      rename(Pais = 1,Region = 2, PIB_FMI=3,p_FMI=4,PIB_ONU=5,p_ONU=6,PIB_BM = 7,
             p_BM = 8) %>% 
      mutate(across(PIB_FMI:p_BM,~gsub(",","",.))) %>% as_tibble()

    PIB_paises
    ```

## Web scrapping de paginas

## Web scrapping de múltiples páginas (páginas anidadas)

## Web scrapping en páginas con consultas

### Cuando hay un "captcha"

## Web scraping en entórnos de JavaCript

# Notas sobre data/wrangling

```{r}

#  Base inicial
nivel_edu <- data.frame(
  region = "America latina",
  pais = c(sample("Colombia",5,T),sample("Peru",5,T),sample("Venezuela",5,T)),
  materia = c("matematicas","lenguaje", "ingles", "fisica", "quimica"),
  p_2014 = rnorm(15,50,15),
  p_2016 = rnorm(15,55,10),
  p_2018 = rnorm(15,54,13),
  p_2020 = rnorm(15,61,8),
  p_2022 = rnorm(15,67,19)) %>% 
  as_tibble()

nivel_edu
```

```{r}
# pivot_longer() cumple la misma función de "melt()"
nivel_edu <- nivel_edu %>% 
  pivot_longer(starts_with("p_"), "periodo",values_to = "notas")
nivel_edu
```

```{r}
# pivot_wider() cumple la misma función que "dcast()"
nivel_edu %>%
  pivot_wider(names_from = materia, values_from = notas)

```
